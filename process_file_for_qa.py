#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
GSW æ–‡ä»¶è™•ç†è…³æœ¬ - è®€å–ã€åˆ‡å¡Šã€èªç¾©æå–ä¸¦ä¿å­˜ä¾›QAä½¿ç”¨

æ­¤è…³æœ¬å°‡æ–‡ä»¶å…§å®¹è®€å–å¾Œé€²è¡Œæ–‡æœ¬åˆ‡å¡Šï¼Œç„¶å¾Œä½¿ç”¨GSWå­¸ç¿’ç³»çµ±
é€²è¡Œèªç¾©æå–ä¸¦ä¿å­˜åˆ°å‘é‡æ•¸æ“šåº«ä¸­ï¼Œä¾›å¾ŒçºŒå•ç­”ä½¿ç”¨ã€‚

ä½¿ç”¨æ–¹æ³•:
python process_file_for_qa.py path/to/your/file.txt
python process_file_for_qa.py file.txt --strategy paragraph --chunk-size 800
"""

import argparse
import sys
import logging
import os
from pathlib import Path

# æ·»åŠ é …ç›®è·¯å¾‘åˆ°sys.path
project_root = Path(__file__).parent / "gsw-learning-mvp"
os.environ.setdefault("GEMINI_MODEL_NAME", "gemini-2.0-flash")
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "src"))

try:
    from src.file_reader import FileReader
    from src.text_chunker import TextChunker
    from src.gsw_learning_system import GSWLearningSystem
except ImportError as e:
    print(f"âŒ å°å…¥æ¨¡çµ„å¤±æ•—: {e}")
    print("è«‹ç¢ºä¿å·²å®‰è£ä¾è³´åŒ…ä¸¦åœ¨æ­£ç¢ºçš„ç›®éŒ„ä¸­é‹è¡Œ")
    sys.exit(1)

# é…ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def process_file_for_qa(file_path: str, chunk_strategy: str = "semantic", chunk_size: int = 1000, overlap: int = 100):
    """
    è™•ç†å–®å€‹æ–‡ä»¶ï¼šè®€å– -> åˆ‡å¡Š -> èªç¾©æå– -> ä¿å­˜

    Args:
        file_path: æ–‡ä»¶è·¯å¾‘
        chunk_strategy: åˆ‡å¡Šç­–ç•¥ ("fixed", "semantic", "paragraph")
        chunk_size: æ¯å€‹chunkçš„å¤§å°
        overlap: chunké–“é‡ç–Šå¤§å°

    Returns:
        bool: è™•ç†æ˜¯å¦æˆåŠŸ
    """
    print(f"ğŸš€ é–‹å§‹è™•ç†æ–‡ä»¶: {file_path}")

    # 1. è®€å–æ–‡ä»¶
    print("\nğŸ“– éšæ®µ1: è®€å–æ–‡ä»¶")
    try:
        reader = FileReader()
        result = reader.read_file(file_path)

        if not result['success']:
            print(f"âŒ æ–‡ä»¶è®€å–å¤±æ•—: {result['error_message']}")
            return False

        content = result['content']
        metadata = result['metadata']
        print(f"âœ“ æ–‡ä»¶è®€å–æˆåŠŸ")
        print(f"  - æ–‡ä»¶å¤§å°: {metadata.file_size} å­—ç¯€")
        print(f"  - ç·¨ç¢¼: {metadata.encoding}")
        print(f"  - å…§å®¹é•·åº¦: {len(content)} å­—ç¬¦")

    except Exception as e:
        print(f"âŒ æ–‡ä»¶è®€å–éç¨‹ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        return False

    # 2. åˆ‡å¡Š
    print("\nâœ‚ï¸  éšæ®µ2: æ–‡æœ¬åˆ‡å¡Š")
    try:
        chunker = TextChunker(
            chunk_size=chunk_size,
            overlap=overlap,
            strategy=chunk_strategy
        )
        chunks = chunker.chunk_text(content)

        print(f"âœ“ æ–‡æœ¬åˆ‡å¡Šå®Œæˆ")
        print(f"  - åˆ‡å¡Šç­–ç•¥: {chunk_strategy}")
        print(f"  - chunkå¤§å°: {chunk_size} å­—ç¬¦")
        print(f"  - é‡ç–Šå¤§å°: {overlap} å­—ç¬¦")
        print(f"  - ç”Ÿæˆchunksæ•¸é‡: {len(chunks)}")

        if chunks:
            print(f"  - ç¬¬ä¸€å€‹chunkå¤§å°: {chunks[0]['chunk_size']} å­—ç¬¦")
            print(f"  - æœ€å¾Œä¸€å€‹chunkå¤§å°: {chunks[-1]['chunk_size']} å­—ç¬¦")

    except Exception as e:
        print(f"âŒ æ–‡æœ¬åˆ‡å¡Šéç¨‹ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        return False

    # 3. åˆå§‹åŒ–GSWç³»çµ±
    print("\nğŸ§  éšæ®µ3: åˆå§‹åŒ–GSWå­¸ç¿’ç³»çµ±")
    try:
        gsw_system = GSWLearningSystem()
        print("âœ“ GSWå­¸ç¿’ç³»çµ±åˆå§‹åŒ–å®Œæˆ")
    except Exception as e:
        print(f"âŒ GSWç³»çµ±åˆå§‹åŒ–å¤±æ•—: {str(e)}")
        print("è«‹æª¢æŸ¥.envé…ç½®æ–‡ä»¶å’ŒAPIå¯†é‘°è¨­ç½®")
        return False

    # 4. è™•ç†æ¯å€‹chunk
    print("\nğŸ’¾ éšæ®µ4: èªç¾©æå–ä¸¦ä¿å­˜")
    processed_count = 0
    failed_count = 0

    for i, chunk in enumerate(chunks, 1):
        print(f"  è™•ç†chunk {i}/{len(chunks)} (å¤§å°: {chunk['chunk_size']} å­—ç¬¦)")

        try:
            # èªç¾©æå–ä¸¦ä¿å­˜
            updated_workspace = gsw_system.process_text(chunk['content'])
            processed_count += 1

            # æ¯è™•ç†10å€‹chunksé¡¯ç¤ºä¸€æ¬¡é€²åº¦
            if i % 10 == 0 or i == len(chunks):
                print(f"âœ“ å·²è™•ç† {processed_count}/{len(chunks)} å€‹chunks")

        except Exception as e:
            print(f"âŒ chunk {i} è™•ç†å¤±æ•—: {str(e)}")
            failed_count += 1
            continue

    # è™•ç†çµæœçµ±è¨ˆ
    print("\nğŸ“Š è™•ç†çµæœçµ±è¨ˆ:")
    print(f"  - ç¸½chunksæ•¸: {len(chunks)}")
    print(f"  - æˆåŠŸè™•ç†: {processed_count}")
    print(f"  - è™•ç†å¤±æ•—: {failed_count}")
    print(f"  - æˆåŠŸç‡: {(processed_count/len(chunks)*100):.1f}%")

    if processed_count > 0:
        print("\nğŸ‰ æ–‡ä»¶è™•ç†å®Œæˆï¼çŸ¥è­˜åº«å·²æ›´æ–°ï¼Œå¯ä»¥é–‹å§‹å•ç­”äº†ï¼")
        return True
    else:
        print("\nâŒ æ‰€æœ‰chunksè™•ç†éƒ½å¤±æ•—äº†ï¼Œè«‹æª¢æŸ¥é…ç½®å’ŒAPIè¨­ç½®")
        return False


def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(
        description="GSW æ–‡ä»¶è™•ç†è…³æœ¬ - è®€å–ã€åˆ‡å¡Šã€èªç¾©æå–ä¸¦ä¿å­˜ä¾›QAä½¿ç”¨",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python process_file_for_qa.py document.txt
  python process_file_for_qa.py document.md --strategy paragraph
  python process_file_for_qa.py document.txt --chunk-size 800 --overlap 50

åˆ‡å¡Šç­–ç•¥:
  fixed     - å›ºå®šå¤§å°åˆ‡å¡Š
  semantic  - èªç¾©é‚Šç•Œåˆ‡å¡Šï¼ˆæ¨è–¦ï¼‰
  paragraph - æ®µè½é‚Šç•Œåˆ‡å¡Š

æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: .txt, .md, .json
        """
    )

    parser.add_argument("file_path", help="è¦è™•ç†çš„æ–‡ä»¶è·¯å¾‘")
    parser.add_argument(
        "--strategy",
        choices=["fixed", "semantic", "paragraph"],
        default="semantic",
        help="åˆ‡å¡Šç­–ç•¥ (é»˜èª: semantic)"
    )
    parser.add_argument(
        "--chunk-size",
        type=int,
        default=1000,
        help="æ¯å€‹chunkçš„å¤§å°ï¼Œå­—ç¬¦æ•¸ (é»˜èª: 1000)"
    )
    parser.add_argument(
        "--overlap",
        type=int,
        default=100,
        help="chunksé–“é‡ç–Šå¤§å°ï¼Œå­—ç¬¦æ•¸ (é»˜èª: 100)"
    )

    args = parser.parse_args()

    # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    file_path = Path(args.file_path)
    if not file_path.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.file_path}")
        sys.exit(1)

    # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦ç‚ºæ”¯æŒçš„æ ¼å¼
    supported_extensions = {'.txt', '.md', '.json'}
    if file_path.suffix.lower() not in supported_extensions:
        print(f"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path.suffix}")
        print(f"æ”¯æŒçš„æ ¼å¼: {', '.join(supported_extensions)}")
        sys.exit(1)

    print("=" * 60)
    print("GSW æ–‡ä»¶è™•ç†è…³æœ¬")
    print("=" * 60)
    print(f"æ–‡ä»¶è·¯å¾‘: {args.file_path}")
    print(f"åˆ‡å¡Šç­–ç•¥: {args.strategy}")
    print(f"chunkå¤§å°: {args.chunk_size} å­—ç¬¦")
    print(f"é‡ç–Šå¤§å°: {args.overlap} å­—ç¬¦")
    print("=" * 60)

    # è™•ç†æ–‡ä»¶
    success = process_file_for_qa(
        str(file_path),
        args.strategy,
        args.chunk_size,
        args.overlap
    )

    print("\n" + "=" * 60)
    if success:
        print("âœ… è…³æœ¬åŸ·è¡ŒæˆåŠŸï¼")
        print("\nğŸ’¡ æç¤º:")
        print("  ç¾åœ¨æ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç¢¼é€²è¡Œå•ç­”:")
        print("  from gsw_learning_mvp.gsw_learning_system import GSWLearningSystem")
        print("  gsw = GSWLearningSystem()")
        print("  answer = gsw.query('æ‚¨çš„å•é¡Œï¼Ÿ')")
    else:
        print("âŒ è…³æœ¬åŸ·è¡Œå¤±æ•—ï¼")
        sys.exit(1)


if __name__ == "__main__":
    main()